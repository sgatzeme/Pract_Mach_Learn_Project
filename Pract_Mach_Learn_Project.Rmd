---
output: html_document
---
Practical Machine Learning Project: Predicting Excercise Manners
==========================================================================================================

### 0. Synopsis

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of this project is to predict the manner in which subjects exercise.

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The outcome variable is classe, a factor variable with 5 levels. For this data set, "participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in 5 different fashions:

- exactly according to the specification (Class A)
- throwing the elbows to the front (Class B)
- lifting the dumbbell only halfway (Class C)
- lowering the dumbbell only halfway (Class D)
- throwing the hips to the front (Class E)"

### 1. Setting the basic parameters

Loading frequently used functions consolidated in the misc.R script file (such as _downloadFile()_ and _getNAStrings()_)
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
source("../common/misc.R")
```

Logging session information for debugging
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
origLocale <- Sys.getlocale("LC_TIME")
session <- sessInfo(clear=TRUE, loc=c("LC_TIME", "English"), log=TRUE)
```

Loading required libraries
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
library(ggplot2)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(knitr)
```

Changing basic R options
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
options(scipen=999)
```

Setting up input and data directory
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
inputDir <- "../Pract_Mach_Learn"
dataDir <- file.path(inputDir,"data")
```

Downloading source files
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
downloadFile(dataDir, "training_set.csv", 
             "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")

downloadFile(dataDir, "testing_set.csv", 
             "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```

Reading in source files
```{r echo=TRUE, message=FALSE, warning=FALSE}
trainFile <- file.path(dataDir, "training_set.csv")
testFile <- file.path(dataDir, "testing_set.csv")

data.train <- read.csv(trainFile, header = TRUE, sep = ",", na.strings = getNaStrings(), stringsAsFactors = FALSE)
data.test <- read.csv(testFile, header = TRUE, sep = ",", na.strings = getNaStrings(), stringsAsFactors = FALSE)
```

Setting seed for reproducibility
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
set.seed(230215)
```

### 2. Preprocessing the data

A quick look into the data revealed, that even though missing values have been detected and replaced with "NA", there are several columns consisting only of missing values. Thus, before being able to build an adequate prediction model, we have to preprocess the data. First, we removed variables with low variance values. Second, we checked for possible duplicate entries and removed the first 6 columns, since timestamp related data as well as ID columns are not helpful for our analysis. Third, we removed entire missing value columns from the data frame. 

```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
data.train.nearzero <- nearZeroVar(data.train, saveMetrics = TRUE)
data.train <- data.train[, !data.train.nearzero$nzv]
data.train <- data.train[!duplicated(data.train), -c(1:6)]
data.train <- data.train[, colSums(is.na(data.train)) == 0]
```

We repeated these steps for the testing set and also removed the "problem_id" column:
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
data.test.nearzero <- nearZeroVar(data.test, saveMetrics = TRUE)
data.test <- data.test[, !data.test.nearzero$nzv]
data.test <- data.test[!duplicated(data.test), -c(1:6)]
data.test <- data.test[, colSums(is.na(data.test)) == 0]
data.test <- data.test[, !(names(data.test) %in% c("problem_id"))]
```

Then, we conducted a correlation analysis in order to find variables with a correlation greater or equal to 0.9.
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
corr.train <- cor(na.omit(data.train[sapply(data.train, is.numeric)]))
corr.remove <- findCorrelation(corr.train, cutoff = .90, verbose = FALSE)

data.train <- data.train[, -corr.remove]
data.test <- data.test[, colnames(data.test) %in% colnames(data.train)]
```

After the preprocessing steps, the training and testing data set are ready for the analysis. We have reduced both data sets to 46 and 45 variables (the testing set does not have a "classe" column).

### Finding the adequate prediction model

There is a wide range of methods to predict the classe variables. Since we would like to solve a classification problem, we have decided to compare the accuracy of random forest, recursive partitioning, naive bayes and suport vector machines. All these approaches provide a good tradeoff between complexity and performance. The chosen prediction method will then be executed for the classification of our outcome variable from the testing set.

Hence, we start our comparison with cross validation and split our training set into a training and testing subset with a ratio of 80/20.
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
split <- createDataPartition(y=data.train$classe, p=0.8, list=FALSE) 
data.train.train <- data.train[split, ] 
data.train.test <- data.train[-split, ]
```

As we can see, the distribution of the training subset class variable is nearly the same as in the training set
```{r echo=TRUE, message=FALSE, warning=FALSE}
qplot(classe, data=data.train.train, geom="histogram")

qplot(classe, data=data.train, geom="histogram")
```

Random forest prediction model
```{r echo=TRUE, message=FALSE, warning=FALSE}
md_rf <- randomForest(as.factor(classe) ~ ., data=data.train.train, method="class")
pred_rf <- predict(md_rf, data.train.test, type = "class")
con_rf <- confusionMatrix(pred_rf, data.train.test$classe)
con_rf
```

Recursive Partitioning prediction model
```{r echo=TRUE, message=FALSE, warning=FALSE}
md_rp <- rpart(as.factor(classe) ~ ., data=data.train.train, method="class")
pred_rp <- predict(md_rp, data.train.test, type = "class")
con_rp <- confusionMatrix(pred_rp, data.train.test$classe)
con_rp
```

Naive bayes prediction model
```{r echo=TRUE, message=FALSE, warning=FALSE}
md_nb <- naiveBayes(as.factor(classe) ~ ., data=data.train.train)
pred_nb <- predict(md_nb, data.train.test, type = "class")
con_nb <- confusionMatrix(pred_nb, data.train.test$classe)
con_nb
```

Support vector machine prediction model
```{r echo=TRUE, message=FALSE, warning=FALSE}
md_sv <- svm(as.factor(classe) ~ ., data=data.train.train, gamma = 0.1)
pred_sv <- predict(md_sv, data.train.test, type = "class")
con_sv <- confusionMatrix(pred_sv, data.train.test$classe)
con_sv
```

In order to summarize the relevant information, we consolidated the prediction_results

```{r echo=TRUE, message=FALSE, warning=FALSE}
pred_results <- data.frame(random_forest=pred_rf,
                           recursive_partitioning=pred_rp,
                           naive_bayes=pred_nb,
                           support_vector_machine=pred_sv                           
                           )

acc_results <- data.frame(model=c("random_forest", "recursive_partitioning", "naive_bayes", "support_vector_machines"),
                          accuracy=c(con_rf$overall[1], con_rp$overall[1], con_nb$overall[1], con_sv$overall[1]))

acc_results <- acc_results[order(-acc_results$accuracy),]

kable(acc_results)
```

The results suggest, that random forest as well as support vector machine prediction models provide the highest accuracy. Therefore, we used these two methods to predict the "classe" variable in the testing set.

```{r echo=TRUE, message=FALSE, warning=FALSE}
final_rf <- predict(md_rf, data.test, type="class")
final_sv <- predict(md_sv, data.test, type="class")
```

When comparing the results, we see that there is only one difference in predicting the excercise classes for the subjects. However, since random forest has a slightly better accuracy, we decided to use these results for the final submission.
```{r echo=TRUE, message=FALSE, warning=FALSE}
final_results <- data.frame(problem_id=c(1:20),
                            random_forest=final_rf,
                            support_vector_machine=final_sv,
                            check=ifelse(final_rf == final_sv,TRUE,FALSE)
                            )

final_results
```

With a custom _writeOutputfiles_ function, we exported the results as .txt files for upload into the coursera portal. 
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
writeOutputfiles = function(x){
    n = length(x)
    for(i in 1:n){
        filename = file.path(dataDir, paste0("case_",i,".txt"))   
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

writeOutputfiles(final_results$random_forest)
```

Finally, we have to set back the originale locale
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
session <- sessInfo(clear=TRUE, loc=c("LC_TIME",origLocale), log=TRUE)
```

### References
Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. [Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements.](http://groupware.les.inf.puc-rio.br/har#ixzz3SL0H9aaT) Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.